import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import label_binarize
import joblib
import os
import yaml

from imblearn.over_sampling import SMOTE, BorderlineSMOTE

class ModelTrainer:
    def __init__(self, config_path="configs/config.yaml", config=None):
        # ç®€åŒ–é…ç½®åŠ è½½ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥ configï¼Œå¦åˆ™ç”¨ utf-8-sig è¯»å– yaml
        if config is not None:
            self.config = config
        else:
            with open(config_path, 'r', encoding='utf-8-sig') as f:
                self.config = yaml.safe_load(f)
        
        self.models = {}
        self.best_params = {}
        self.cv_results = {}
        # å¦‚æœ config ä¸­åŒ…å« training.epochsï¼Œåˆ™å­˜å‚¨æ–¹ä¾¿ä½¿ç”¨
        # ä¸å¯¹ epochs åšç‰¹æ®Šè¦†ç›–å¤„ç†ï¼Œä¿æŒé…ç½®æ–‡ä»¶ä¸ºå•ä¸€äº‹å®æº
        
        # åˆ›å»ºæ¨¡å‹ä¿å­˜ç›®å½•
        os.makedirs('models', exist_ok=True)
    
    def initialize_models(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨æ¨¡å‹"""
        model_config = self.config['models']
        classifiers = model_config.get('classifiers', [])

        for name in classifiers:
            params = self.config['models'].get(name, {})
            n_est = params.get('n_estimators', None)
            if isinstance(n_est, list) and len(n_est) > 0:
                try:
                    n_est = int(n_est[0])
                except Exception:
                    n_est = None


            if name == 'random_forest':
                if n_est:
                    self.models['random_forest'] = RandomForestClassifier(n_estimators=n_est, random_state=self.config['training']['random_state'], class_weight='balanced')
                else:
                    self.models['random_forest'] = RandomForestClassifier(random_state=self.config['training']['random_state'], class_weight='balanced')

            if name == 'adaboost':
                if n_est:
                    self.models['adaboost'] = AdaBoostClassifier(n_estimators=n_est, random_state=self.config['training']['random_state'])
                else:
                    self.models['adaboost'] = AdaBoostClassifier(random_state=self.config['training']['random_state'])
        
        print(f"å·²åˆå§‹åŒ–æ¨¡å‹: {list(self.models.keys())}")
    
    def perform_grid_search(self, X_train, y_train, model_name, task_name):
        """æ‰§è¡Œç½‘æ ¼æœç´¢è¿›è¡Œå‚æ•°è°ƒä¼˜"""
        print(f"\nå¼€å§‹å¯¹ {model_name} è¿›è¡Œå‚æ•°è°ƒä¼˜ ({task_name})...")
        if model_name not in self.models:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}")
        # ä»é…ç½®ä¸­æ„å»º param_gridï¼ˆå‘åå…¼å®¹ï¼šä½¿ç”¨ models.<name> ä¸‹çš„å‚æ•°åˆ—è¡¨ï¼‰
        model_cfg = self.config.get('models', {}).get(model_name, {})
        param_grid = {}

        if model_name == 'random_forest':
            for k in ('n_estimators', 'max_depth', 'min_samples_split'):
                if k in model_cfg:
                    param_grid[k] = model_cfg[k]

        elif model_name == 'adaboost':
            for k in ('n_estimators', 'learning_rate'):
                if k in model_cfg:
                    param_grid[k] = model_cfg[k]

        # å¦‚æœæ²¡æœ‰åœ¨æ—§é…ç½®ä¸­æ‰¾åˆ° param_gridï¼Œå°è¯•è¯»å–ç›´æ¥ç»™å‡ºçš„ param_grid å­—æ®µï¼ˆæ–°æ ¼å¼ï¼‰
        if not param_grid:
            param_grid = model_cfg.get('param_grid', {})

        if not param_grid:
            # æ²¡æœ‰ç½‘æ ¼ï¼Œç›´æ¥åœ¨åˆå§‹åŒ–æ¨¡å‹ä¸Šè®­ç»ƒå¹¶è¿”å›
            self.models[model_name].fit(X_train, y_train)
            return self.models[model_name]

        cv_folds = model_cfg.get('cv_folds', self.config.get('training', {}).get('cv_folds', 5))
        scoring = self.config.get('training', {}).get('scoring_metric', None)

        grid_search = GridSearchCV(estimator=self.models[model_name], param_grid=param_grid, cv=cv_folds, scoring=scoring, n_jobs=-1, verbose=1)
        grid_search.fit(X_train, y_train)

        # ä¿å­˜ç»“æœ
        self.best_params[f"{task_name}_{model_name}"] = grid_search.best_params_
        self.cv_results[f"{task_name}_{model_name}"] = grid_search.cv_results_
        print(f"æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        try:
            print(f"æœ€ä½³äº¤å‰éªŒè¯åˆ†æ•°: {grid_search.best_score_:.4f}")
        except Exception:
            pass
        return grid_search.best_estimator_
    
    def train_with_cross_validation(self, X_train, y_train, model_name, task_name):
        """ä½¿ç”¨äº¤å‰éªŒè¯è®­ç»ƒæ¨¡å‹"""

        #æ–°å¢ï¼šæ•°æ®éªŒè¯
        if len(X_train) == 0 or len(y_train) == 0:
            raise ValueError("è®­ç»ƒæ•°æ®ä¸ºç©º")
    
        if len(np.unique(y_train)) < 2:
            raise ValueError(f"ç±»åˆ«æ•°é‡ä¸è¶³ï¼Œæ— æ³•è®­ç»ƒæ¨¡å‹: {np.unique(y_train)}")
    
        print(f"å¼€å§‹è®­ç»ƒ {model_name} - {task_name}")
        print(f"è®­ç»ƒæ•°æ®å½¢çŠ¶: X={len(X_train)}, y={len(y_train)}")
        print(f"ç±»åˆ«åˆ†å¸ƒ: {np.bincount(y_train)}")

        if self.config['training']['use_cross_validation']:
            # æ‰§è¡Œç½‘æ ¼æœç´¢å¹¶è¿”å›æœ€ä½³ estimatorï¼ˆperform_grid_search å·²è®­ç»ƒå¹¶è¿”å›ï¼‰
            best_model = self.perform_grid_search(X_train, y_train, model_name, task_name)

        else:
            # ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
            best_model = self.models[model_name]
            best_model.fit(X_train, y_train)

        return best_model
    
    def evaluate_model(self, model, X_test, y_test, model_name, task_name):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆå«æ¯ç±»ç²¾åº¦/å¬å›/F1/AUCï¼‰"""
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test) if hasattr(model, 'predict_proba') else None

        classes = sorted(np.unique(y_test))

        # æ•´ä½“æŒ‡æ ‡ï¼ˆåŠ æƒå¹³å‡ï¼‰
        accuracy = accuracy_score(y_test, y_pred)
        precision_w = precision_score(y_test, y_pred, average='weighted', zero_division=0)
        recall_w = recall_score(y_test, y_pred, average='weighted', zero_division=0)
        f1_w = f1_score(y_test, y_pred, average='weighted', zero_division=0)

        # åˆ†ç±»æŠ¥å‘Šè·å–æ¯ç±» precision/recall/f1/support
        report = classification_report(
            y_test,
            y_pred,
            labels=classes,
            output_dict=True,
            zero_division=0
        )

        # æ¯ç±»å‡†ç¡®ç‡ï¼ˆOne-vs-Rest çš„ (TP+TN)/Nï¼‰
        cm = confusion_matrix(y_test, y_pred, labels=classes)
        total = cm.sum()
        per_class = {}
        for idx, cls in enumerate(classes):
            tp = cm[idx, idx]
            fn = cm[idx, :].sum() - tp
            fp = cm[:, idx].sum() - tp
            tn = total - tp - fn - fp
            class_acc = (tp + tn) / total if total > 0 else 0.0

            cls_key = str(cls)
            per_class[cls] = {
                'accuracy': class_acc,
                'precision': report.get(cls_key, {}).get('precision', 0.0),
                'recall': report.get(cls_key, {}).get('recall', 0.0),
                'f1_score': report.get(cls_key, {}).get('f1-score', 0.0),
                'support': report.get(cls_key, {}).get('support', 0)
            }

        # æ¯ç±» AUCï¼ˆå¦‚æœ‰æ¦‚ç‡è¾“å‡ºï¼‰
        auc_macro = None
        if y_pred_proba is not None:
            try:
                y_true_bin = label_binarize(y_test, classes=classes)
                if y_pred_proba.shape[1] == len(classes):
                    per_class_auc = roc_auc_score(y_true_bin, y_pred_proba, average=None)
                    auc_macro = roc_auc_score(y_true_bin, y_pred_proba, average='macro')
                    for i, cls in enumerate(classes):
                        per_class[cls]['auc'] = per_class_auc[i]
                else:
                    for cls in classes:
                        per_class[cls]['auc'] = None
            except Exception:
                # æ¦‚ç‡æ— æ³•ç”¨äº AUC æ—¶ä¿æŒ None
                for cls in classes:
                    per_class[cls]['auc'] = None
        else:
            for cls in classes:
                per_class[cls]['auc'] = None

        metrics = {
            # ä¿æŒå‘åå…¼å®¹çš„æ‰å¹³å­—æ®µ
            'accuracy': accuracy,
            'precision': precision_w,
            'recall': recall_w,
            'f1_score': f1_w,
            # åˆ†ç»„å­—æ®µ
            'overall': {
                'accuracy': accuracy,
                'precision_weighted': precision_w,
                'recall_weighted': recall_w,
                'f1_weighted': f1_w,
                'macro_precision': report.get('macro avg', {}).get('precision', 0.0),
                'macro_recall': report.get('macro avg', {}).get('recall', 0.0),
                'macro_f1': report.get('macro avg', {}).get('f1-score', 0.0),
                'auc_macro': auc_macro
            },
            'per_class': per_class
        }

        print(f"\n{model_name} åœ¨ {task_name} ä¸Šçš„è¡¨ç°:")
        print(f"  accuracy: {accuracy:.4f}")
        print(f"  precision (weighted): {precision_w:.4f}")
        print(f"  recall (weighted): {recall_w:.4f}")
        print(f"  f1_score (weighted): {f1_w:.4f}")

        return {
            'model': model,
            'y_test': y_test,
            'y_pred': y_pred,
            'y_pred_proba': y_pred_proba,
            'metrics': metrics
        }
    
    def train_all_models(self, features, df, task_columns=None):
        """è®­ç»ƒæ‰€æœ‰æ¨¡å‹ï¼ˆé»˜è®¤ä»…é’ˆå¯¹é»„æ–‘æ°´è‚¿é£é™©åˆ†ç±»ï¼Œé›†æˆSMOTEé‡é‡‡æ ·ï¼‰"""
        if task_columns is None:
            task_columns = ['Risk of macular edema']

        results = {}
        resample_cfg = self.config.get('resampling', {})
        use_smote = resample_cfg.get('use_smote', True)
        smote_type = resample_cfg.get('smote_type', 'borderline')
        smote_random_state = resample_cfg.get('random_state', 42)
        smote_k = resample_cfg.get('k_neighbors', 5)
        extreme_oversample = resample_cfg.get('extreme_oversample', False)
        minority_target_ratio = resample_cfg.get('minority_target_ratio', 3)

        for task_column in task_columns:
            print(f"\n{'='*50}")
            print(f"å¼€å§‹è®­ç»ƒ {task_column} åˆ†ç±»æ¨¡å‹ï¼ˆSMOTEé‡é‡‡æ ·:{use_smote}ï¼‰")
            print(f"{'='*50}")

            labels = df[task_column].values

            # åˆ†å‰²æ•°æ®
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                features,
                labels,
                test_size=self.config['training']['test_size'],
                random_state=self.config['training']['random_state'],
                stratify=labels
            )

            # ä»…å¯¹è®­ç»ƒé›†åšSMOTE
            if use_smote:
                print("å¯¹è®­ç»ƒé›†è¿›è¡ŒSMOTEè¿‡é‡‡æ ·...")
                if smote_type == 'borderline':
                    smote = BorderlineSMOTE(random_state=smote_random_state, k_neighbors=smote_k)
                else:
                    smote = SMOTE(random_state=smote_random_state, k_neighbors=smote_k)
                X_train, y_train = smote.fit_resample(X_train, y_train)

                print(f"SMOTEåè®­ç»ƒé›†æ ·æœ¬åˆ†å¸ƒ: {np.bincount(y_train)}")

                # æç«¯è¿‡é‡‡æ ·ï¼šå°†å°ç±»æ ·æœ¬æ•°æå‡åˆ°ä¸»ç±»çš„3å€
                if extreme_oversample:
                    from collections import Counter
                    y_counts = Counter(y_train)
                    max_class = max(y_counts, key=lambda k: y_counts[k])
                    for cls in y_counts:
                        if cls == max_class:
                            continue
                        n_target = y_counts[max_class] * minority_target_ratio
                        idxs = np.where(y_train == cls)[0]
                        n_repeat = int(np.ceil(n_target / len(idxs)))
                        X_aug = np.repeat(X_train[idxs], n_repeat, axis=0)[:n_target]
                        y_aug = np.repeat(y_train[idxs], n_repeat, axis=0)[:n_target]
                        X_train = np.concatenate([X_train, X_aug], axis=0)
                        y_train = np.concatenate([y_train, y_aug], axis=0)
                    print(f"æç«¯è¿‡é‡‡æ ·åè®­ç»ƒé›†æ ·æœ¬åˆ†å¸ƒ: {np.bincount(y_train)}")

            task_results = {}

            for model_name in self.models.keys():
                print(f"\nè®­ç»ƒ {model_name} æ¨¡å‹...")

                # è®­ç»ƒæ¨¡å‹
                trained_model = self.train_with_cross_validation(
                    X_train, y_train, model_name, task_column
                )

                # è¯„ä¼°æ¨¡å‹
                model_result = self.evaluate_model(
                    trained_model, X_test, y_test, model_name, task_column
                )

                task_results[model_name] = model_result

            results[task_column] = {
                'model_results': task_results,
                'X_test': X_test,
                'y_test': y_test,
                'X_train': X_train,
                'y_train': y_train
            }

        return results
    
    def save_models(self, results, save_dir="models"):
        """ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹"""
        for task_name, task_data in results.items():
            for model_name, model_info in task_data['model_results'].items():
                filename = os.path.join(save_dir, f"{task_name}_{model_name}.pkl")
                joblib.dump(model_info['model'], filename)
                print(f"æ¨¡å‹å·²ä¿å­˜: {filename}")
        
        # ä¿å­˜å‚æ•°å’Œç»“æœ
        results_filename = os.path.join(save_dir, "training_results.pkl")
        joblib.dump({
            'best_params': self.best_params,
            'cv_results': self.cv_results
        }, results_filename)

    #æ–°å¢ï¼šåœ¨ModelTrainerç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•
    def compare_models(self, results):
        """æ¯”è¾ƒä¸åŒæ¨¡å‹çš„æ€§èƒ½"""
        comparison_data = []
    
        for task_name, task_data in results.items():
            for model_name, model_info in task_data['model_results'].items():
                metrics = model_info['metrics']
                comparison_data.append({
                    'Task': task_name,
                    'Model': model_name,
                    'Accuracy': metrics['accuracy'],
                    'F1-Score': metrics['f1_score'],
                    'Precision': metrics['precision'],
                    'Recall': metrics['recall']
                })
    
        comparison_df = pd.DataFrame(comparison_data)
    
        # æ‰¾å‡ºæ¯ä¸ªä»»åŠ¡çš„æœ€ä½³æ¨¡å‹
        best_models = {}
        for task in comparison_df['Task'].unique():
            task_df = comparison_df[comparison_df['Task'] == task]
            best_idx = task_df['F1-Score'].idxmax()
            best_models[task] = task_df.loc[best_idx]
    
        print("\nğŸ† æœ€ä½³æ¨¡å‹è¯„é€‰:")
        for task, model_info in best_models.items():
            print(f"{task}: {model_info['Model']} (F1: {model_info['F1-Score']:.4f})")
    
        return comparison_df, best_models